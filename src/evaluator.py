import pandas as pd
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import cross_val_score
from config import cfg

class Evaluator:
    def run(self, data, models):
        print("\nðŸ“Š [Task 5] å¼€å§‹æ¨¡åž‹æ€§èƒ½è¯„ä¼°...")
        X_train, y_train = data['X_train'], data['y_train']
        X_test, y_test = data['X_test'], data['y_test']
        X_full, y_full = data['X_full'], data['y_full'] # ç”¨äºŽCV

        results = []
        
        print(f"{'Algorithm':<25} | {'Test Acc':<10} | {'CV Mean':<10}")
        print("-" * 50)
        
        for name, model in models.items():
            # 1. è®­ç»ƒä¸Žæµ‹è¯•
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            acc = accuracy_score(y_test, y_pred)
            
            # 2. äº¤å‰éªŒè¯
            cv_scores = cross_val_score(model, X_full, y_full, cv=5)
            cv_mean = cv_scores.mean()
            
            print(f"{name:<25} | {acc:.4f}     | {cv_mean:.4f}")
            results.append({'Model': name, 'Test Acc': acc, 'CV Mean': cv_mean})
        
        # ä¿å­˜åˆ°CSV
        df = pd.DataFrame(results)
        df.to_csv(f"{cfg.OUTPUT_DIR}/performance_metrics.csv", index=False)
        print(f"\nâœ… è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜è‡³ {cfg.OUTPUT_DIR}/performance_metrics.csv")